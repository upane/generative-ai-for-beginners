[![開放原始碼模型](../../img/18-lesson-banner.png?WT.mc_id=academic-105485-koreyst)](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst)

# 微調您的 LLM

使用大型語言模型來建構生成式 AI 應用程式會帶來新的挑戰。一個關鍵問題是確保模型針對特定使用者請求所生成內容的品質（準確性和相關性）。在之前的課程中，我們討論了提示工程和檢索增強生成等技術，這些技術試圖通過_修改提示輸入_來解決現有模型的問題。

在今天的課程中，我們討論了第三種技術，**微調**，這種技術試圖通過_重新訓練模型本身_並使用額外的數據來解決這一挑戰。讓我們深入了解細節。

## 學習目標

本課程介紹了對預訓練語言模型進行微調的概念，探討了這種方法的優點和挑戰，並提供了何時以及如何使用微調來提高生成式 AI 模型性能的指導。

在本課程結束時，你應該能夠回答以下問題:

- 語言模型的微調是什麼？
- 微調在什麼時候以及為什麼有用？
- 我如何微調一個預訓練模型？
- 微調的限制是什麼？

準備好了嗎？讓我們開始吧。

## 圖解指南

想要在深入探討之前了解我們將涵蓋的整體內容嗎？查看這個描述本課程學習旅程的圖解指南 - 從學習核心概念和微調的動機，到了解執行微調任務的過程和最佳實踐。這是一個引人入勝的探索主題，所以別忘了查看[資源](../../RESOURCES.md?WT.mc_id=academic-105485-koreyst)頁面以獲取更多支持自學旅程的連結！

![語言模型微調圖解指南](../../img/18-fine-tuning-sketchnote.png?WT.mc_id=academic-105485-koreyst)

## 語言模型的微調是什麼？

根據定義，大型語言模型是從包括互聯網在內的多種來源獲取的大量文本上進行_預訓練_的。正如我們在之前的課程中所學到的，我們需要像_提示工程_和_檢索增強生成_這樣的技術來提高模型對用戶問題（「提示」）的回應品質。

一個流行的提示工程技術涉及通過提供_指示_（明確指導）或_給它一些範例_（隱含指導）來給模型更多關於回應預期的指導。這被稱為_少量學習_，但它有兩個限制:

- 模型標記限制會限制您能提供的範例數量，並限制效果。
- 模型標記成本會使在每個提示中添加範例變得昂貴，並限制靈活性。

微調是機器學習系統中的一種常見做法，我們採用預訓練模型並使用新數據重新訓練它，以提高其在特定任務上的性能。在語言模型的背景下，我們可以微調預訓練模型_使用為給定任務或應用領域精選的一組範例_來建立一個**自訂模型**，這個模型可能對於該特定任務或領域更準確和相關。微調的一個附帶好處是，它還可以減少少量學習所需的範例數量——減少 token 使用和相關成本。

## 何時以及為什麼我們應該微調模型？

在 _此_ 上下文中，當我們談論微調時，我們指的是**監督式**微調，其中重新訓練是通過**添加新數據**來完成的，這些數據不是原始訓練數據集的一部分。這與無監督微調方法不同，後者是在原始數據上重新訓練模型，但使用不同的超參數。

關鍵是要記住，微調是一種需要一定專業知識才能獲得預期結果的高級技術。如果操作不當，它可能不會提供預期的改進，甚至可能降低模型在您目標領域的性能。

所以，在你學習「如何」微調語言模型之前，你需要知道「為什麼」你應該選擇這條路，以及「何時」開始微調的過程。首先問自己這些問題:

- **使用案例**: 你的微調_使用案例_是什麼？你想改進當前預訓練模型的哪個方面？
- **替代方案**: 你是否嘗試過_其他技術_來達到預期結果？使用它們來建立比較的基準。
  - 提示工程: 嘗試使用相關提示回應範例的少量提示技術。評估回應的品質。
  - 檢索增強生成: 嘗試使用檢索數據查詢結果來增強提示。評估回應的品質。
- **成本**: 你是否已經識別出微調的成本？
  - 可調性 - 預訓練模型是否可用於微調？
  - 努力 - 準備訓練數據、評估和改進模型的努力。
  - 計算 - 執行微調工作和部署微調模型的計算成本
  - 數據 - 是否有足夠品質的範例來影響微調效果
- **收益**: 你是否確認了微調的收益？
  - 品質 - 微調後的模型是否超越了基準？
  - 成本 - 是否通過簡化提示來減少代幣使用？
  - 可擴展性 - 你能否將基礎模型重新用於新領域？

藉由回答這些問題，你應該能夠決定微調是否適合你的使用案例。理想情況下，只有當收益超過成本時，這種方法才是有效的。一旦你決定繼續，就該考慮_如何_微調預訓練模型。

想要獲得更多關於決策過程的見解嗎？觀看[是否要微調](https://www.youtube.com/watch?v=0Jo-z-MFxJs)

## 我們如何微調一個預訓練模型？

要微調一個預訓練模型，你需要具備:

- 一個預訓練模型來微調
- 一個數據集來用於微調
- 一個訓練環境來執行微調工作
- 一個託管環境來部署微調後的模型

## 微調操作中

以下資源提供逐步指南，帶您通過使用精選資料集的選定模型進行真實範例。要完成這些指南，您需要在特定提供者上擁有一個帳戶，並訪問相關的模型和資料集。

| Provider     | Tutorial                                                                                                                                                                       | Description                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| OpenAI       | [How to fine-tune chat models](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb?WT.mc_id=academic-105485-koreyst)                | 學習如何為特定領域（"秘訣助手"）微調 `gpt-35-turbo`，包括準備訓練資料、執行微調工作以及使用微調後的模型進行推論。                                                                                                                                                                                                                                              |
| Azure OpenAI | [GPT 3.5 Turbo fine-tuning tutorial](https://learn.microsoft.com/azure/ai-services/openai/tutorials/fine-tune?tabs=python-new%2Ccommand-line?WT.mc_id=academic-105485-koreyst) | 學習如何在 **Azure** 上微調 `gpt-35-turbo-0613` 模型，包括建立和上傳訓練資料、執行微調工作。部署並使用新模型。                                                                                                                                                                                                                                                                 |
| Hugging Face | [Fine-tuning LLMs with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl?WT.mc_id=academic-105485-koreyst)                                               | 這篇博客文章介紹如何使用 [transformers](https://huggingface.co/docs/transformers/index?WT.mc_id=academic-105485-koreyst) 函式庫和 [Transformer Reinforcement Learning (TRL)](https://huggingface.co/docs/trl/index?WT.mc_id=academic-105485-koreyst) 以及 Hugging Face 上的開放 [datasets](https://huggingface.co/docs/datasets/index?WT.mc_id=academic-105485-koreyst) 來微調 _open LLM_（例如：`CodeLlama 7B`）。 |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| 🤗 AutoTrain | [Fine-tuning LLMs with AutoTrain](https://github.com/huggingface/autotrain-advanced/?WT.mc_id=academic-105485-koreyst)                                                         | AutoTrain（或 AutoTrain Advanced）是由 Hugging Face 開發的 python 函式庫，允許對許多不同的任務進行微調，包括 LLM 微調。AutoTrain 是一個無需程式碼的解決方案，可以在自己的雲端、Hugging Face Spaces 或本地進行微調。它支持基於網頁的 GUI、CLI 和通過 yaml 配置文件進行訓練。                                                                               |
|              |                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                    |

## 作業

選擇上面的其中一個指南並逐步完成它們。_我們可能會在這個 Repo 中的 Jupyter Notebooks 中複製這些指南的版本僅供參考。請直接使用原始來源以獲取最新版本_。

## 很棒的工作！繼續學習。

完成本課程後，請查看我們的[生成式 AI 學習集合](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst)以繼續提升您的生成式 AI 知識！

恭喜!! 你已經完成了這門課程 v2 系列的最後一課! 不要停止學習和建構。**查看[資源](RESOURCES.md?WT.mc_id=academic-105485-koreyst)頁面，獲取更多關於此主題的建議。

我們的 v1 系列課程也已更新了更多的作業和概念。所以花點時間重新整理你的知識 - 並且請[分享你的問題和反饋](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst)，以幫助我們改進這些課程，造福社群。

